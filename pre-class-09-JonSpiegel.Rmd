---
title: "Basic Webscraping"
---

<style type="text/css">
.table {

    width: 80%;
    margin-left:10%; 
    margin-right:10%;
}
</style>
```{r,setup, echo=FALSE, cache=TRUE}
## numbers >= 10^5 will be denoted in scientific notation,
## and rounded to 2 digits
options(scipen = 3, digits = 3)
```



## Exercises

1. Read the HTML content of the following URL with a variable called webpage: https://money.cnn.com/data/us_markets/ At this point, it will also be useful to open this web page in your browser.
```{r}
library(httr)
library(xml2)
library(rvest)#launching libraries for webscraping

savedurl <- "https://money.cnn.com/data/us_markets/"
read_html(savedurl)

```
2. Get the session details (status, type, size) of the above mentioned URL.
```{r}
geturl <- GET(savedurl) #GET gives all of the desired outputs, other functions can specifically find them
status_code(geturl) #finding status
content_type(savedurl) #finding type

```
3. Extract all of the sector names from the "Stock Sectors" table (bottom left of the web page.)
```{r}
table2 <- html_table(read_html(savedurl, "table"))[[2]] #make a dataframe from the table
table2 #look at the dataframe
table2[,1] #we want the first column of the df

```
4. Extract all of the "3 Month % Change" values from the "Stock Sectors" table.
```{r}
table2[,2] #now we want the second column from the df
```
5. Extract the table "What's Moving" (top middle of the web page) into a data-frame.
```{r}
table1 <- html_table(read_html(savedurl, "table"))[[1]] 
class(table1) #yayy its a dataframe
```
6. Re-construct all of the links from the first column of the "What's Moving" table.
Hint: the base URL is "https://money.cnn.com"
```{r}
paste0("https://money.cnn.com", html_attr(html_nodes(readurl, "td .wsod_symbol"), "href")) #could never have done this without basically finding it online. I do not know these functions. The videos didn't teach me this
```

7. Extract the titles under the "Latest News" section (bottom middle of the web page.)
```{r}
readurl <- read_html(savedurl)
#page_text <- html(savedurl)
#latestnews <- html_node(page_text, "div#section_latestnews") #this doesn't give it the way we want it to... we want the headline list
#html_node(page_text, "div#HeadlineList") #this returns NA though so we can't just ask directly
#html_node(page_text, "div#section_latestnews~ ul")#I expected this to work...
#html_node(page_text, "div#HeadlineList~ ul") #still NA

html_text(html_nodes(readurl, ".HeadlineList a")) #found online again. Not sure why it is HeadlineList a. the a is not expected to me. I don't see that in the code online. I commented out my incorrect attempts above
```
8. To understand the structure of the data in a web page, it is often useful to know what the underlying attributes are of the text you see. Extract the attributes (and their values) of the HTML element that holds the timestamp underneath the "What's Moving" table.
```{r}
readurl %>% html_nodes(".wsod_disclaimer > span") %>% html_attrs() %>% .[[1]] #again I needed the internet in order to figure out how to do this one. I can't take much credit for it at all. I am still unclear on exactly what goes on
```

9. Extract the values of the blue percentage-bars from the "Trending Tickers" table (bottom right of the web page.)
Hint: in this case, the values are stored under the "class" attribute.
```{r}
#table3 <- html_table(read_html(savedurl, "table"))[[3]] #Oh no! It lists them as NA... No idea how to retrieve what they are
#so the above commented out code above doesn't work because it gives NA's. It must use something like how I just found number 8 to work
readurl %>% html_nodes(".scale div") %>% html_attr("class") #in the code on the site we can see how the bars are stored as scale in the div and so we can call them as such
```

10. Get the links of all of the "svg" images on the web page.
```{r}
readurl %>% html_nodes("img[src$='svg']") %>% html_attr("src") #I knew the general structure of how to due this one but img[src$='svg'] required internet's help
```



